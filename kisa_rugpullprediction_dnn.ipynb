{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kisa_rugpullprediction_dnn.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [
        "MGnaqy8PkOiL",
        "rG9xGT_pkUVx",
        "a5Ac8d4NmNAG",
        "5CuJsiDpqevi",
        "lnzRomALrklZ",
        "jNGuaNbCqCuX"
      ],
      "authorship_tag": "ABX9TyM4p5lVaY1I8g9ipWH7xKE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NesquikPlz/kisa_rugpullprediction/blob/main/kisa_rugpullprediction_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Package implementation"
      ],
      "metadata": {
        "id": "MGnaqy8PkOiL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQm65GIWnh4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64660bb6-859b-435f-87ca-6fde7eb4e651"
      },
      "source": [
        "pip install keras-tuner --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4pRK0dLn8kR",
        "outputId": "d8e49bb7-0625-4033-c98a-64cc2ff82dca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpIMQ_dGpLpy"
      },
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the dataset"
      ],
      "metadata": {
        "id": "rG9xGT_pkUVx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "SOtKFHdUn8nC",
        "outputId": "18387de5-5a83-4799-db19-619e167ba542"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/dataset/Dataset_v1.10.csv')\n",
        "train=train.sample(frac=1).reset_index(drop=True) #shuffle the data\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b8b539ed-a1c7-4523-afd4-b479551b77cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Label</th>\n",
              "      <th>mint_count_per_week</th>\n",
              "      <th>burn_count_per_week</th>\n",
              "      <th>mint_ratio</th>\n",
              "      <th>swap_ratio</th>\n",
              "      <th>burn_ratio</th>\n",
              "      <th>mint_mean_period</th>\n",
              "      <th>swap_mean_period</th>\n",
              "      <th>burn_mean_period</th>\n",
              "      <th>swapIn_per_week</th>\n",
              "      <th>swapOut_per_week</th>\n",
              "      <th>swap_rate</th>\n",
              "      <th>LP_avg</th>\n",
              "      <th>LP_stdev</th>\n",
              "      <th>LPCreator_holding_ratio</th>\n",
              "      <th>Lock_ratio</th>\n",
              "      <th>token_burn_ratio</th>\n",
              "      <th>Creator_token_holding_ratio</th>\n",
              "      <th>number_of_token_creation_of_Creator</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0xb7a9b1587e36e7d91d55b832e62a026e2d10a547</td>\n",
              "      <td>True</td>\n",
              "      <td>0.994987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.499836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.949872</td>\n",
              "      <td>4.974936</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-9.410000e-14</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.616847e-01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0x5fbe56e9a55e19f8012f453e3c0e5126c89ae383</td>\n",
              "      <td>True</td>\n",
              "      <td>5.942579</td>\n",
              "      <td>1.697880</td>\n",
              "      <td>0.114754</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.032787</td>\n",
              "      <td>0.160875</td>\n",
              "      <td>0.233660</td>\n",
              "      <td>0.829332</td>\n",
              "      <td>29.712893</td>\n",
              "      <td>14.431977</td>\n",
              "      <td>1.944444</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.495</td>\n",
              "      <td>2.585587e-01</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0xd3156495b10b89571927994d6f2d2f6133e5431b</td>\n",
              "      <td>True</td>\n",
              "      <td>0.988549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014493</td>\n",
              "      <td>0.985507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.286326</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>49.427433</td>\n",
              "      <td>17.793876</td>\n",
              "      <td>2.631579</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-4.880000e-12</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.150000e-01</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0x27f4196ed4d649942d48870e1c66447060c219d8</td>\n",
              "      <td>True</td>\n",
              "      <td>0.956195</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.654780</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.255314</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.620000e-13</td>\n",
              "      <td>0.000</td>\n",
              "      <td>9.635000e-01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0x0ef1da42084230aac38bdbea4cb624ba0152e120</td>\n",
              "      <td>False</td>\n",
              "      <td>0.882540</td>\n",
              "      <td>0.882540</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.969231</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044462</td>\n",
              "      <td>0.998397</td>\n",
              "      <td>40.596823</td>\n",
              "      <td>15.003174</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.500</td>\n",
              "      <td>1.285051e-01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18291</th>\n",
              "      <td>0x7660881d3124a831c90a3f96777f4255f16bd920</td>\n",
              "      <td>False</td>\n",
              "      <td>0.381558</td>\n",
              "      <td>0.152623</td>\n",
              "      <td>0.007874</td>\n",
              "      <td>0.988976</td>\n",
              "      <td>0.003150</td>\n",
              "      <td>0.008192</td>\n",
              "      <td>0.086247</td>\n",
              "      <td>0.357710</td>\n",
              "      <td>28.006324</td>\n",
              "      <td>19.917304</td>\n",
              "      <td>1.400763</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>44.897741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.489774e-01</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.871971e-02</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18292</th>\n",
              "      <td>0x3c640b7d611a6ab6cd17d54b759e216c41d26f29</td>\n",
              "      <td>True</td>\n",
              "      <td>0.970073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.977273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.952568</td>\n",
              "      <td>7.760587</td>\n",
              "      <td>3.888889</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.350000e-12</td>\n",
              "      <td>0.000</td>\n",
              "      <td>5.448603e-01</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18293</th>\n",
              "      <td>0x354aac0614105cfa47a04c11f0283db3e46a1fa8</td>\n",
              "      <td>True</td>\n",
              "      <td>0.889714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037037</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115048</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.132564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.240000e-16</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.060000e-16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18294</th>\n",
              "      <td>0xe32479d25b6cb8c02507c3568813e11a37fa32ca</td>\n",
              "      <td>False</td>\n",
              "      <td>10.224145</td>\n",
              "      <td>6.523527</td>\n",
              "      <td>0.061022</td>\n",
              "      <td>0.900042</td>\n",
              "      <td>0.038935</td>\n",
              "      <td>0.059137</td>\n",
              "      <td>0.123956</td>\n",
              "      <td>0.154623</td>\n",
              "      <td>76.384567</td>\n",
              "      <td>74.415648</td>\n",
              "      <td>1.026131</td>\n",
              "      <td>2.564103</td>\n",
              "      <td>14.614641</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.264112e-01</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18295</th>\n",
              "      <td>0xc2d43c7553561aec4696f326be0b8e9730a2635f</td>\n",
              "      <td>True</td>\n",
              "      <td>0.999450</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009709</td>\n",
              "      <td>0.990291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.756757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>101.943870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-2.630000e-11</td>\n",
              "      <td>0.000</td>\n",
              "      <td>7.360000e-01</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18296 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8b539ed-a1c7-4523-afd4-b479551b77cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8b539ed-a1c7-4523-afd4-b479551b77cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8b539ed-a1c7-4523-afd4-b479551b77cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               id  ...  number_of_token_creation_of_Creator\n",
              "0      0xb7a9b1587e36e7d91d55b832e62a026e2d10a547  ...                                    1\n",
              "1      0x5fbe56e9a55e19f8012f453e3c0e5126c89ae383  ...                                    6\n",
              "2      0xd3156495b10b89571927994d6f2d2f6133e5431b  ...                                    3\n",
              "3      0x27f4196ed4d649942d48870e1c66447060c219d8  ...                                    1\n",
              "4      0x0ef1da42084230aac38bdbea4cb624ba0152e120  ...                                    1\n",
              "...                                           ...  ...                                  ...\n",
              "18291  0x7660881d3124a831c90a3f96777f4255f16bd920  ...                                    2\n",
              "18292  0x3c640b7d611a6ab6cd17d54b759e216c41d26f29  ...                                   26\n",
              "18293  0x354aac0614105cfa47a04c11f0283db3e46a1fa8  ...                                    1\n",
              "18294  0xe32479d25b6cb8c02507c3568813e11a37fa32ca  ...                                    1\n",
              "18295  0xc2d43c7553561aec4696f326be0b8e9730a2635f  ...                                    2\n",
              "\n",
              "[18296 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uii5pImmPeV",
        "outputId": "f6ec4476-75c5-4bee-c94f-4e4a5216dabe"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'Label', 'mint_count_per_week', 'burn_count_per_week',\n",
              "       'mint_ratio', 'swap_ratio', 'burn_ratio', 'mint_mean_period',\n",
              "       'swap_mean_period', 'burn_mean_period', 'swapIn_per_week',\n",
              "       'swapOut_per_week', 'swap_rate', 'LP_avg', 'LP_stdev',\n",
              "       'LPCreator_holding_ratio', 'Lock_ratio', 'token_burn_ratio',\n",
              "       'Creator_token_holding_ratio', 'number_of_token_creation_of_Creator'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irR8Ldo0naFJ"
      },
      "source": [
        "train = train.drop(columns = ['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux36CPiTd5c8"
      },
      "source": [
        "# train = train.drop(columns = ['id', 'swap_count'])\n",
        "train['Label'] = train['Label'].replace({True : 1, False : 0})\n",
        "train = train.dropna(how='any',axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA9QgAq2ph1D"
      },
      "source": [
        "x = train.drop(columns = ['Label'])\n",
        "y = train['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpqO9Y3duMLP"
      },
      "source": [
        "#data normalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x[ : ] = scaler.fit_transform(x[ : ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "1m2aFE5DuPXu",
        "outputId": "84803322-9b39-40a6-84af-be89e61b8ed1"
      },
      "source": [
        "x = x.astype(np.float32)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-99b4d64f-84cc-4867-9146-e084787ef77b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mint_count_per_week</th>\n",
              "      <th>burn_count_per_week</th>\n",
              "      <th>mint_ratio</th>\n",
              "      <th>swap_ratio</th>\n",
              "      <th>burn_ratio</th>\n",
              "      <th>mint_mean_period</th>\n",
              "      <th>swap_mean_period</th>\n",
              "      <th>burn_mean_period</th>\n",
              "      <th>swapIn_per_week</th>\n",
              "      <th>swapOut_per_week</th>\n",
              "      <th>swap_rate</th>\n",
              "      <th>LP_avg</th>\n",
              "      <th>LP_stdev</th>\n",
              "      <th>LPCreator_holding_ratio</th>\n",
              "      <th>Lock_ratio</th>\n",
              "      <th>token_burn_ratio</th>\n",
              "      <th>Creator_token_holding_ratio</th>\n",
              "      <th>number_of_token_creation_of_Creator</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062411</td>\n",
              "      <td>0.937606</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.499836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005705</td>\n",
              "      <td>0.004927</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074172</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.033036</td>\n",
              "      <td>0.009719</td>\n",
              "      <td>0.114670</td>\n",
              "      <td>0.852555</td>\n",
              "      <td>0.032890</td>\n",
              "      <td>0.176635</td>\n",
              "      <td>0.233660</td>\n",
              "      <td>0.829332</td>\n",
              "      <td>0.017035</td>\n",
              "      <td>0.014294</td>\n",
              "      <td>0.001786</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041715</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.004978</td>\n",
              "      <td>0.074112</td>\n",
              "      <td>0.019920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.005409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014399</td>\n",
              "      <td>0.985619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.286326</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028338</td>\n",
              "      <td>0.017623</td>\n",
              "      <td>0.002417</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074165</td>\n",
              "      <td>0.007968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055466</td>\n",
              "      <td>0.944551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.654780</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015611</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074217</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.004817</td>\n",
              "      <td>0.005052</td>\n",
              "      <td>0.015291</td>\n",
              "      <td>0.969340</td>\n",
              "      <td>0.015433</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044462</td>\n",
              "      <td>0.998397</td>\n",
              "      <td>0.023276</td>\n",
              "      <td>0.014859</td>\n",
              "      <td>0.002347</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041715</td>\n",
              "      <td>0.975182</td>\n",
              "      <td>0.005028</td>\n",
              "      <td>0.074093</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18291</th>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.007780</td>\n",
              "      <td>0.989088</td>\n",
              "      <td>0.003159</td>\n",
              "      <td>0.008995</td>\n",
              "      <td>0.086247</td>\n",
              "      <td>0.357710</td>\n",
              "      <td>0.016057</td>\n",
              "      <td>0.019726</td>\n",
              "      <td>0.001286</td>\n",
              "      <td>0.499400</td>\n",
              "      <td>0.897955</td>\n",
              "      <td>0.041715</td>\n",
              "      <td>0.946916</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074081</td>\n",
              "      <td>0.003984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18292</th>\n",
              "      <td>0.005306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022635</td>\n",
              "      <td>0.977383</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019466</td>\n",
              "      <td>0.007686</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074155</td>\n",
              "      <td>0.099602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18293</th>\n",
              "      <td>0.004857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036946</td>\n",
              "      <td>0.963072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115048</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023875</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074074</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18294</th>\n",
              "      <td>0.056913</td>\n",
              "      <td>0.037341</td>\n",
              "      <td>0.060933</td>\n",
              "      <td>0.900144</td>\n",
              "      <td>0.039057</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.123956</td>\n",
              "      <td>0.154623</td>\n",
              "      <td>0.043794</td>\n",
              "      <td>0.073702</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.024473</td>\n",
              "      <td>0.292293</td>\n",
              "      <td>0.041715</td>\n",
              "      <td>0.934415</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074074</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18295</th>\n",
              "      <td>0.005469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.990403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.756757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058448</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093664</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972849</td>\n",
              "      <td>0.421204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074183</td>\n",
              "      <td>0.003984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18296 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99b4d64f-84cc-4867-9146-e084787ef77b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99b4d64f-84cc-4867-9146-e084787ef77b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99b4d64f-84cc-4867-9146-e084787ef77b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       mint_count_per_week  ...  number_of_token_creation_of_Creator\n",
              "0                 0.005445  ...                             0.000000\n",
              "1                 0.033036  ...                             0.019920\n",
              "2                 0.005409  ...                             0.007968\n",
              "3                 0.005228  ...                             0.000000\n",
              "4                 0.004817  ...                             0.000000\n",
              "...                    ...  ...                                  ...\n",
              "18291             0.002024  ...                             0.003984\n",
              "18292             0.005306  ...                             0.099602\n",
              "18293             0.004857  ...                             0.000000\n",
              "18294             0.056913  ...                             0.000000\n",
              "18295             0.005469  ...                             0.003984\n",
              "\n",
              "[18296 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2T3Zk2xpp-Z"
      },
      "source": [
        "#train data : test data = 8:2\n",
        "x_train, x_val, y_train, y_val= train_test_split(x, y, test_size=0.2, random_state=777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNN Model"
      ],
      "metadata": {
        "id": "a5Ac8d4NmNAG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2KVvjssstjs"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n",
        " \n",
        "# Initializing the ANN\n",
        "model1 = Sequential()\n",
        "num = 124\n",
        "# Adding the input layer and the first hidden Layer\n",
        "model1.add(Dense(units=num, kernel_initializer='uniform', activation='relu', input_dim=18))\n",
        "\n",
        "model1.add(Dense(units=2*num, kernel_initializer='uniform', activation='relu'))\n",
        "\n",
        "model1.add(Dense(units=4*num, kernel_initializer='uniform', activation='relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(units=8*num, kernel_initializer='uniform', activation='relu'))\n",
        "\n",
        "model1.add(Dense(units=4*num, kernel_initializer='uniform', activation='relu'))\n",
        "\n",
        "model1.add(Dense(units=2*num, kernel_initializer='uniform', activation='relu'))\n",
        "\n",
        "model1.add(Dense(units=num, kernel_initializer='uniform', activation='relu'))\n",
        " \n",
        "# Adding the output layer\n",
        "model1.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCngbYHlstmu"
      },
      "source": [
        "model1.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss = 'binary_crossentropy',\n",
        "            metrics=['accuracy'],\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AE8U1stOMlO"
      },
      "source": [
        "model1.save('ann97.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA1mKhI8qXcr",
        "outputId": "e74f80e4-0d55-4c32-ebe9-f3ed7a9259a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 124)               2356      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 248)               31000     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 496)               123504    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 496)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 992)               493024    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 496)               492528    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 248)               123256    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 124)               30876     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 125       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,296,669\n",
            "Trainable params: 1,296,669\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "5CuJsiDpqevi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqJ2KUvnstpx",
        "outputId": "c402972f-ac5d-4e2d-8324-1ceeda489633"
      },
      "source": [
        "history = model1.fit(x_train, y_train, epochs=96, validation_split=0.2, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/96\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 0.3228 - accuracy: 0.8988 - val_loss: 0.1814 - val_accuracy: 0.8904\n",
            "Epoch 2/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1708 - accuracy: 0.9155 - val_loss: 0.1579 - val_accuracy: 0.9436\n",
            "Epoch 3/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.1443 - accuracy: 0.9488 - val_loss: 0.1435 - val_accuracy: 0.9464\n",
            "Epoch 4/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.1298 - accuracy: 0.9570 - val_loss: 0.1302 - val_accuracy: 0.9563\n",
            "Epoch 5/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1242 - accuracy: 0.9612 - val_loss: 0.1266 - val_accuracy: 0.9614\n",
            "Epoch 6/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.1172 - accuracy: 0.9634 - val_loss: 0.1205 - val_accuracy: 0.9617\n",
            "Epoch 7/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1153 - accuracy: 0.9646 - val_loss: 0.1243 - val_accuracy: 0.9621\n",
            "Epoch 8/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1149 - accuracy: 0.9646 - val_loss: 0.1201 - val_accuracy: 0.9631\n",
            "Epoch 9/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.1121 - accuracy: 0.9669 - val_loss: 0.1225 - val_accuracy: 0.9614\n",
            "Epoch 10/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1109 - accuracy: 0.9683 - val_loss: 0.1212 - val_accuracy: 0.9638\n",
            "Epoch 11/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1110 - accuracy: 0.9669 - val_loss: 0.1185 - val_accuracy: 0.9645\n",
            "Epoch 12/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1091 - accuracy: 0.9671 - val_loss: 0.1170 - val_accuracy: 0.9631\n",
            "Epoch 13/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.1014 - accuracy: 0.9703 - val_loss: 0.1178 - val_accuracy: 0.9638\n",
            "Epoch 14/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.1055 - accuracy: 0.9687 - val_loss: 0.1131 - val_accuracy: 0.9658\n",
            "Epoch 15/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.1013 - accuracy: 0.9689 - val_loss: 0.1163 - val_accuracy: 0.9665\n",
            "Epoch 16/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0965 - accuracy: 0.9706 - val_loss: 0.1156 - val_accuracy: 0.9645\n",
            "Epoch 17/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0975 - accuracy: 0.9694 - val_loss: 0.1077 - val_accuracy: 0.9658\n",
            "Epoch 18/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.1011 - accuracy: 0.9690 - val_loss: 0.1089 - val_accuracy: 0.9655\n",
            "Epoch 19/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0972 - accuracy: 0.9697 - val_loss: 0.1201 - val_accuracy: 0.9679\n",
            "Epoch 20/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0975 - accuracy: 0.9705 - val_loss: 0.1122 - val_accuracy: 0.9676\n",
            "Epoch 21/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0962 - accuracy: 0.9703 - val_loss: 0.1166 - val_accuracy: 0.9594\n",
            "Epoch 22/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0962 - accuracy: 0.9712 - val_loss: 0.1084 - val_accuracy: 0.9624\n",
            "Epoch 23/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0906 - accuracy: 0.9724 - val_loss: 0.1030 - val_accuracy: 0.9676\n",
            "Epoch 24/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0904 - accuracy: 0.9728 - val_loss: 0.0993 - val_accuracy: 0.9699\n",
            "Epoch 25/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0926 - accuracy: 0.9708 - val_loss: 0.1190 - val_accuracy: 0.9686\n",
            "Epoch 26/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0894 - accuracy: 0.9716 - val_loss: 0.1117 - val_accuracy: 0.9597\n",
            "Epoch 27/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0904 - accuracy: 0.9721 - val_loss: 0.1028 - val_accuracy: 0.9672\n",
            "Epoch 28/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0850 - accuracy: 0.9729 - val_loss: 0.0999 - val_accuracy: 0.9710\n",
            "Epoch 29/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0807 - accuracy: 0.9728 - val_loss: 0.1044 - val_accuracy: 0.9699\n",
            "Epoch 30/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0802 - accuracy: 0.9740 - val_loss: 0.0994 - val_accuracy: 0.9706\n",
            "Epoch 31/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0864 - accuracy: 0.9740 - val_loss: 0.1013 - val_accuracy: 0.9696\n",
            "Epoch 32/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0831 - accuracy: 0.9739 - val_loss: 0.1091 - val_accuracy: 0.9679\n",
            "Epoch 33/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0811 - accuracy: 0.9742 - val_loss: 0.1019 - val_accuracy: 0.9720\n",
            "Epoch 34/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0775 - accuracy: 0.9758 - val_loss: 0.1072 - val_accuracy: 0.9693\n",
            "Epoch 35/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0835 - accuracy: 0.9731 - val_loss: 0.0967 - val_accuracy: 0.9723\n",
            "Epoch 36/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0788 - accuracy: 0.9733 - val_loss: 0.0945 - val_accuracy: 0.9710\n",
            "Epoch 37/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.1006 - val_accuracy: 0.9672\n",
            "Epoch 38/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0811 - accuracy: 0.9745 - val_loss: 0.0919 - val_accuracy: 0.9727\n",
            "Epoch 39/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0776 - accuracy: 0.9741 - val_loss: 0.0989 - val_accuracy: 0.9699\n",
            "Epoch 40/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0763 - accuracy: 0.9756 - val_loss: 0.0980 - val_accuracy: 0.9696\n",
            "Epoch 41/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0750 - accuracy: 0.9762 - val_loss: 0.0904 - val_accuracy: 0.9734\n",
            "Epoch 42/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0740 - accuracy: 0.9766 - val_loss: 0.0947 - val_accuracy: 0.9713\n",
            "Epoch 43/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0790 - accuracy: 0.9749 - val_loss: 0.0937 - val_accuracy: 0.9703\n",
            "Epoch 44/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0750 - accuracy: 0.9760 - val_loss: 0.0957 - val_accuracy: 0.9720\n",
            "Epoch 45/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0784 - accuracy: 0.9746 - val_loss: 0.0991 - val_accuracy: 0.9693\n",
            "Epoch 46/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0775 - accuracy: 0.9759 - val_loss: 0.0996 - val_accuracy: 0.9696\n",
            "Epoch 47/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0744 - accuracy: 0.9766 - val_loss: 0.1052 - val_accuracy: 0.9682\n",
            "Epoch 48/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0765 - accuracy: 0.9757 - val_loss: 0.0988 - val_accuracy: 0.9696\n",
            "Epoch 49/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0738 - accuracy: 0.9757 - val_loss: 0.0915 - val_accuracy: 0.9706\n",
            "Epoch 50/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0752 - accuracy: 0.9751 - val_loss: 0.1015 - val_accuracy: 0.9672\n",
            "Epoch 51/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0724 - accuracy: 0.9764 - val_loss: 0.0945 - val_accuracy: 0.9740\n",
            "Epoch 52/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0728 - accuracy: 0.9758 - val_loss: 0.0979 - val_accuracy: 0.9734\n",
            "Epoch 53/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0763 - accuracy: 0.9758 - val_loss: 0.0914 - val_accuracy: 0.9710\n",
            "Epoch 54/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0723 - accuracy: 0.9763 - val_loss: 0.1129 - val_accuracy: 0.9672\n",
            "Epoch 55/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0752 - accuracy: 0.9760 - val_loss: 0.1213 - val_accuracy: 0.9662\n",
            "Epoch 56/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0741 - accuracy: 0.9760 - val_loss: 0.1154 - val_accuracy: 0.9696\n",
            "Epoch 57/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0735 - accuracy: 0.9759 - val_loss: 0.1039 - val_accuracy: 0.9703\n",
            "Epoch 58/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0697 - accuracy: 0.9768 - val_loss: 0.0908 - val_accuracy: 0.9723\n",
            "Epoch 59/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0719 - accuracy: 0.9775 - val_loss: 0.0969 - val_accuracy: 0.9710\n",
            "Epoch 60/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0703 - accuracy: 0.9772 - val_loss: 0.1027 - val_accuracy: 0.9696\n",
            "Epoch 61/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0729 - accuracy: 0.9756 - val_loss: 0.0949 - val_accuracy: 0.9706\n",
            "Epoch 62/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0704 - accuracy: 0.9781 - val_loss: 0.1103 - val_accuracy: 0.9679\n",
            "Epoch 63/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0708 - accuracy: 0.9757 - val_loss: 0.1032 - val_accuracy: 0.9699\n",
            "Epoch 64/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0676 - accuracy: 0.9781 - val_loss: 0.1016 - val_accuracy: 0.9699\n",
            "Epoch 65/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0659 - accuracy: 0.9780 - val_loss: 0.0977 - val_accuracy: 0.9703\n",
            "Epoch 66/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0658 - accuracy: 0.9786 - val_loss: 0.1064 - val_accuracy: 0.9720\n",
            "Epoch 67/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0659 - accuracy: 0.9787 - val_loss: 0.1046 - val_accuracy: 0.9730\n",
            "Epoch 68/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0671 - accuracy: 0.9775 - val_loss: 0.0985 - val_accuracy: 0.9720\n",
            "Epoch 69/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0676 - accuracy: 0.9777 - val_loss: 0.0955 - val_accuracy: 0.9713\n",
            "Epoch 70/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0659 - accuracy: 0.9782 - val_loss: 0.1037 - val_accuracy: 0.9693\n",
            "Epoch 71/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0679 - accuracy: 0.9775 - val_loss: 0.0941 - val_accuracy: 0.9723\n",
            "Epoch 72/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0668 - accuracy: 0.9778 - val_loss: 0.1068 - val_accuracy: 0.9696\n",
            "Epoch 73/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0694 - accuracy: 0.9769 - val_loss: 0.1025 - val_accuracy: 0.9734\n",
            "Epoch 74/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0637 - accuracy: 0.9792 - val_loss: 0.1071 - val_accuracy: 0.9676\n",
            "Epoch 75/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0642 - accuracy: 0.9795 - val_loss: 0.1033 - val_accuracy: 0.9669\n",
            "Epoch 76/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0673 - accuracy: 0.9778 - val_loss: 0.0929 - val_accuracy: 0.9740\n",
            "Epoch 77/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0661 - accuracy: 0.9767 - val_loss: 0.1118 - val_accuracy: 0.9655\n",
            "Epoch 78/96\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 0.0619 - accuracy: 0.9792 - val_loss: 0.1271 - val_accuracy: 0.9679\n",
            "Epoch 79/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0646 - accuracy: 0.9780 - val_loss: 0.1053 - val_accuracy: 0.9730\n",
            "Epoch 80/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0620 - accuracy: 0.9797 - val_loss: 0.1042 - val_accuracy: 0.9720\n",
            "Epoch 81/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0629 - accuracy: 0.9793 - val_loss: 0.1133 - val_accuracy: 0.9655\n",
            "Epoch 82/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0647 - accuracy: 0.9787 - val_loss: 0.1359 - val_accuracy: 0.9720\n",
            "Epoch 83/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0650 - accuracy: 0.9791 - val_loss: 0.1107 - val_accuracy: 0.9737\n",
            "Epoch 84/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0626 - accuracy: 0.9785 - val_loss: 0.1187 - val_accuracy: 0.9730\n",
            "Epoch 85/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0621 - accuracy: 0.9801 - val_loss: 0.1111 - val_accuracy: 0.9699\n",
            "Epoch 86/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.1168 - val_accuracy: 0.9710\n",
            "Epoch 87/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0625 - accuracy: 0.9798 - val_loss: 0.1108 - val_accuracy: 0.9717\n",
            "Epoch 88/96\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.0617 - accuracy: 0.9795 - val_loss: 0.1237 - val_accuracy: 0.9665\n",
            "Epoch 89/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0638 - accuracy: 0.9787 - val_loss: 0.1141 - val_accuracy: 0.9682\n",
            "Epoch 90/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0633 - accuracy: 0.9792 - val_loss: 0.1236 - val_accuracy: 0.9686\n",
            "Epoch 91/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0646 - accuracy: 0.9784 - val_loss: 0.1107 - val_accuracy: 0.9734\n",
            "Epoch 92/96\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 0.0624 - accuracy: 0.9799 - val_loss: 0.1036 - val_accuracy: 0.9706\n",
            "Epoch 93/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0625 - accuracy: 0.9800 - val_loss: 0.1073 - val_accuracy: 0.9713\n",
            "Epoch 94/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0607 - accuracy: 0.9802 - val_loss: 0.0978 - val_accuracy: 0.9758\n",
            "Epoch 95/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0628 - accuracy: 0.9797 - val_loss: 0.1101 - val_accuracy: 0.9710\n",
            "Epoch 96/96\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.0600 - accuracy: 0.9809 - val_loss: 0.1070 - val_accuracy: 0.9734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL37zEtVUNpN"
      },
      "source": [
        "result = model1.predict(x_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYLqOo3O6FGs"
      },
      "source": [
        "def acc_graph(history):\n",
        "    from pylab import rcParams\n",
        "    from matplotlib import pyplot as plt\n",
        "    rcParams['figure.figsize'] = 5, 3\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# 모델 정확도, 예측값, 실제값 출력\n",
        "def evaluate(model, test_X, test_Y, batch_size=100):\n",
        "    evaluation = model.evaluate(test_X,test_Y, batch_size=batch_size)\n",
        "    print(\"acc : \", evaluation[1])\n",
        "    pred = pd.DataFrame(model.predict(test_X))\n",
        "    pred = pred.round(decimals=2)\n",
        "    print(pred)\n",
        "    print(test_Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "4kYE9u4M6Qtq",
        "outputId": "9a046062-3e16-4986-90d2-d65478b6d12e"
      },
      "source": [
        "acc_graph(history)\n",
        "evaluate(model1, x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9765\n",
            "acc :  0.9765027165412903\n",
            "         0\n",
            "0     1.00\n",
            "1     1.00\n",
            "2     1.00\n",
            "3     1.00\n",
            "4     1.00\n",
            "...    ...\n",
            "3655  1.00\n",
            "3656  1.00\n",
            "3657  0.95\n",
            "3658  0.99\n",
            "3659  1.00\n",
            "\n",
            "[3660 rows x 1 columns]\n",
            "11234    1\n",
            "6761     1\n",
            "8691     1\n",
            "3861     1\n",
            "533      1\n",
            "        ..\n",
            "4841     1\n",
            "13454    1\n",
            "16240    1\n",
            "8425     1\n",
            "9935     1\n",
            "Name: Label, Length: 3660, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-G_E_DnH0QY"
      },
      "source": [
        "pred = pd.DataFrame(data= result)\n",
        "pred\n",
        "pred = pred[0].apply(lambda x : 1 if x >=0.5 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkb7WHyEGq6r",
        "outputId": "eb9d473f-cf10-4e99-b992-4eac8dfa3a4c"
      },
      "source": [
        "#evaluate the model : precision, recall, f1score (accuracy is in the above)\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "# pred = model1.predict(x_val,  verbose=0)\n",
        "p = precision_score(y_val, pred)\n",
        "print(p)\n",
        "r = recall_score(y_val, pred)\n",
        "print(r)\n",
        "f1 = f1_score(y_val, pred)\n",
        "print(f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9861570869696058\n",
            "0.9879409104612602\n",
            "0.9870481927710845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyper parameter tuning"
      ],
      "metadata": {
        "id": "lnzRomALrklZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le87dTtHP5c8"
      },
      "source": [
        "!pip install \"ipython>=7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-FJfhfSUy-a"
      },
      "source": [
        "from IPython import display\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeCZzvC1Lvx3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "#내 dnn모델이랑 똑같이 모델 만들어준거\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  hp_units = hp.Int('units', min_value=92, max_value=128, step=4) #여기서 min value랑 max value 설정해서 무슨파라미터가 성능 젤 좋은지 체크할 수 있음\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=2*hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=4*hp_units, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=8*hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=4*hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=2*hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  # Add next layers\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  hp_learning_rate = hp.Choice('learning_rate',values = [0.05, 0.01, 0.005, 0.001])   #learning rate도 이중에 뭐가 젤 좋은지 알고싶다\n",
        "  \n",
        "  opt = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "  model.compile(optimizer= opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\t\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "GVhNZKduQwty",
        "outputId": "c4a79d37-19d2-4e50-ca2d-9e03bc3fc9b0"
      },
      "source": [
        "# 케라스튜너로 hyperparameter tuning을 돌려줍니다\n",
        "\n",
        "tuner = kt.Hyperband(model_builder, \n",
        "                    objective='val_accuracy',\n",
        "                    max_epochs=10,\n",
        "                    factor=3,\n",
        "                    directory = '2me',\n",
        "                    project_name='bobai5')\n",
        "\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "최적화된 첫 번째 Dense 노드 수 : {best_hp.get('units')} \n",
        "최적화된  learnig rate : {best_hp.get('learning_rate')} 입니다.\n",
        "\"\"\")\n",
        "\n",
        "# best 파라미터로 모델 형성\n",
        "model = tuner.hypermodel.build(best_hp)\n",
        "model.fit(x_train, y_train, epochs=10, validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 21 Complete [00h 00m 40s]\n",
            "val_accuracy: 0.9704918265342712\n",
            "\n",
            "Best val_accuracy So Far: 0.9704918265342712\n",
            "Total elapsed time: 00h 09m 24s\n",
            "\n",
            "Search: Running Trial #22\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |96                |128               \n",
            "learning_rate     |0.01              |0.001             \n",
            "tuner/epochs      |4                 |4                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |1                 |1                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/4\n",
            "458/458 [==============================] - 6s 12ms/step - loss: 0.2033 - accuracy: 0.9192 - val_loss: 0.1595 - val_accuracy: 0.9063\n",
            "Epoch 2/4\n",
            "458/458 [==============================] - 5s 11ms/step - loss: 0.1806 - accuracy: 0.9424 - val_loss: 0.1612 - val_accuracy: 0.9434\n",
            "Epoch 3/4\n",
            "458/458 [==============================] - 5s 11ms/step - loss: 0.1821 - accuracy: 0.9440 - val_loss: 0.1415 - val_accuracy: 0.9385\n",
            "Epoch 4/4\n",
            "458/458 [==============================] - 5s 11ms/step - loss: 0.2771 - accuracy: 0.9294 - val_loss: 0.1513 - val_accuracy: 0.9281\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1fb94776f65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mbest_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# objective left unspecified,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDfuHrrasty4"
      },
      "source": [
        "#그리고 여기서 나온 젤 좋은 hyper parameter가지고 모델 파라미터 수정해주면됨\n",
        "best_hp=tuner.get_best_hyperparameters()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FKjryV68HpV"
      },
      "source": [
        "print(best_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction"
      ],
      "metadata": {
        "id": "jNGuaNbCqCuX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTbyqXpLoD-"
      },
      "source": [
        "#Assume that the data(that we gonna predict) are preprocessed\n",
        "#and we call that data 'x_pred'\n",
        "y_pred = model1.predict(x_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmwRrtZ8Oj0E",
        "outputId": "447878be-f8b5-44f4-f4e5-48f01e516e2d"
      },
      "source": [
        "pred = pd.DataFrame(data= y_pred)\n",
        "pred\n",
        "pred = pred[0].apply(lambda x : 1 if x >=0.5 else 0) #if the ai score is higher than 0.5, that coin is malicious\n",
        "\n",
        "#'pred' is a label, and 'y_pred' is the ai score\n",
        "predicted_data = x_pred\n",
        "predicted_data['label'] = pred\n",
        "predicted_data['predictionscore'] = y_pred\n",
        "predicted_data.to_csv('predctionresult.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    }
  ]
}